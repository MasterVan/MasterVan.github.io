<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Kaggle实战——泰坦尼克号生还预测]]></title>
      <url>%2F2017%2F04%2F25%2Fkaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E7%94%9F%E8%BF%98%E9%A2%84%E6%B5%8B%2F</url>
      <content type="text"><![CDATA[数据分析train.csv的属性有： 属性名 定义 取值 PassengerId 乘客编号 1-891 Suvived 生还情况 0, 1 Pclass 票的等级 1,2,3 Name 乘客姓名 Braund, Mr. Owen Harris Sex 性别 male，female Age 年龄 数字，有缺失值 SibSp 兄弟姐妹/配偶在船上 0-8 Parch 父母/子女在船上 0-6 Ticket 船票编号 A/5 21171 Fare 票价 7.25 Cabin 船舱号 C85，有缺失值 Embark 登船港 S,C,Q test.csv缺少Survived字段，也是需要我们预测的 数据预处理1234567import warningswarnings.filterwarnings('ignore')import pandas as pd import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 预览数据123train = pd.read_csv("train.csv")test = pd.read_csv('test.csv')train.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.6+ KB 定义dummies函数dummies函数可将某个离散型特征的所有取值变为one-hot编码12345678def dummies(col,train,test): train_dum = pd.get_dummies(train[col]) test_dum = pd.get_dummies(test[col]) train = pd.concat([train, train_dum], axis=1) test = pd.concat([test,test_dum],axis=1) train.drop(col,axis=1,inplace=True) test.drop(col,axis=1,inplace=True) return train, test 删除无用特征删除PassengerId，Name, Ticket1234# get rid of the useless colsdropping = [&apos;PassengerId&apos;, &apos;Name&apos;, &apos;Ticket&apos;]train.drop(dropping,axis=1, inplace=True)test.drop(dropping,axis=1, inplace=True) Pclass处理观察Pclass和survived的关系，等级越高，生还率越大将Pclass分解为1,2,3三个特征1234print(train.Pclass.value_counts())sns.factorplot("Pclass",'Survived',data=train,order=[1,2,3])train, test = dummies('Pclass',train,test) 3 491 1 216 2 184 Name: Pclass, dtype: int64 Sex处理观察Sex和Survived的关系，女性生还率显著高于男性分解Sex为male，female，并删除原特征12345print(train.Sex.value_counts(dropna=False))sns.factorplot('Sex','Survived',data=train)train,test = dummies('Sex',train,test)train.drop('male',axis=1,inplace=True)test.drop('male',axis=1,inplace=True) male 577 female 314 Name: Sex, dtype: int64 Age处理处理缺失值，计算平均值和方差，对缺失值进行填充观察Age和Survived的关系，在15到30区间对结果影响较大，增加两个特征，Age小于15和Age大于15且小于30，删除Age 12345678910111213141516171819202122232425262728293031323334353637nan_num = len(train[train['Age'].isnull()])age_mean = train['Age'].mean()age_std = train['Age'].std()filling = np.random.randint(age_mean-age_std,age_mean+age_std,size=nan_num)train['Age'][train['Age'].isnull()==True] = fillingnan_num = train['Age'].isnull().sum()# dealing the missing val in testnan_num = test['Age'].isnull().sum()# 86 nullage_mean = test['Age'].mean()age_std = test['Age'].std()filling = np.random.randint(age_mean-age_std,age_mean+age_std,size=nan_num)test['Age'][test['Age'].isnull()==True]=fillingnan_num = test['Age'].isnull().sum()s = sns.FacetGrid(train,hue='Survived',aspect=2)s.map(sns.kdeplot,'Age',shade=True)s.set(xlim=(0,train['Age'].max()))s.add_legend()def under15(row): result = 0.0 if row&lt;15: result = 1.0 return resultdef young(row): result = 0.0 if row&gt;=15 and row&lt;30: result = 1.0 return resulttrain['under15'] = train['Age'].apply(under15)train['young'] = train['Age'].apply(young)test['under15'] = test['Age'].apply(under15)test['young'] = test['Age'].apply(young)train.drop('Age',axis=1,inplace=True)test.drop('Age',axis=1,inplace=True) SibSp和Parch处理发现两者值越大，生还率越低生成组合特征family = SibSp+Parch，删除原特征 1234567891011print (train.SibSp.value_counts(dropna=False))print (train.Parch.value_counts(dropna=False))sns.factorplot('SibSp','Survived',data=train,size=5)sns.factorplot('Parch','Survived',data=train,szie=5)train['family'] = train['SibSp'] + train['Parch']test['family'] = test['SibSp'] + test['Parch']sns.factorplot('family','Survived',data=train,size=5)train.drop(['SibSp','Parch'],axis=1,inplace=True)test.drop(['SibSp','Parch'],axis=1,inplace=True) 0 608 1 209 2 28 4 18 3 16 8 7 5 5 Name: SibSp, dtype: int64 0 678 1 118 2 80 5 5 3 5 4 4 6 1 Name: Parch, dtype: int64 Fare处理票价高的生还率较大，test里有一个缺失值，用均值填充12345678910train.Fare.isnull().sum()test.Fare.isnull().sum()sns.factorplot('Survived','Fare',data=train,size=4)s = sns.FacetGrid(train,hue='Survived',aspect=2)s.map(sns.kdeplot,'Fare',shade=True)s.set(xlim=(0,train['Fare'].max()))s.add_legend()test['Fare'].fillna(test['Fare'].median(),inplace=True) Cabin处理缺失值过多，删除该特征123456#Cabinprint train.Cabin.isnull().sum()print test.Cabin.isnull().sum()train.drop('Cabin',axis=1,inplace=True)test.drop('Cabin',axis=1,inplace=True) 687 327 Embarked处理训练集有两个缺失值，S出现最多，用S进行填充观察发现C港口的乘客生还率较高，分解Embarked为S, Q, C删除S，Q，Embarked. 保留C作为新特征123456789101112#Embarkedprint train.Embarked.isnull().sum()print test.Embarked.isnull().sum()print train['Embarked'].value_counts(dropna=False)train['Embarked'].fillna('S',inplace=True)sns.factorplot('Embarked','Survived',data=train,size=5)train,test = dummies('Embarked',train,test)train.drop(['S','Q'],axis=1,inplace=True)test.drop(['S','Q'],axis=1,inplace=True) 2 0 S 644 C 168 Q 77 NaN 2 Name: Embarked, dtype: int64 训练模型模型选择主要用逻辑回归，随机森林，支持向量机和k近邻 12345678910111213141516171819202122232425262728293031323334353637from sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVC, LinearSVCfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_score, KFolddef modeling(clf,ft,target): acc = cross_val_score(clf,ft,target,cv=kf) acc_lst.append(acc.mean()) return accuracy = []def ml(ft,target,time): accuracy.append(acc_lst) #logisticregression logreg = LogisticRegression() modeling(logreg,ft,target) #RandomForest rf = RandomForestClassifier(n_estimators=50,min_samples_split=4,min_samples_leaf=2) modeling(rf,ft,target) #svc svc = SVC() modeling(svc,ft,target) #knn knn = KNeighborsClassifier(n_neighbors = 3) modeling(knn,ft,target) # see the coefficient logreg.fit(ft,target) feature = pd.DataFrame(ft.columns) feature.columns = ['Features'] feature["Coefficient Estimate"] = pd.Series(logreg.coef_[0]) print(feature) return 使用不同特征组合方案1.使用全部特征 1234567#test1train_ft = train.drop('Survived',axis=1)train_y = train['Survived']kf = KFold(n_splits=3,random_state=1)acc_lst = []ml(train_ft,train_y,'test_1') Features Coefficient Estimate 0 Fare 0.004240 1 1 0.389135 2 2 -0.211795 3 3 -1.210494 4 female 2.689013 5 under15 1.658023 6 young 0.030681 7 family -0.310545 8 C 0.374100 2.删除young 123456789# testing 2, lose youngtrain_ft_2=train.drop(['Survived','young'],axis=1)test_2 = test.drop('young',axis=1)train_ft.head()# mlkf = KFold(n_splits=3,random_state=1)acc_lst=[]ml(train_ft_2,train_y,'test_2') Features Coefficient Estimate 0 Fare 0.004285 1 1 0.386195 2 2 -0.207867 3 3 -1.202922 4 female 2.690898 5 under15 1.645827 6 family -0.311682 7 C 0.376629 3.删除young，C123456789#test3, lose young, ctrain_ft_3=train.drop(['Survived','young','C'],axis=1)test_3 = test.drop(['young','C'],axis=1)train_ft.head()# mlkf = KFold(n_splits=3,random_state=1)acc_lst = []ml(train_ft_3,train_y,'test_3') Features Coefficient Estimate 0 Fare 0.004920 1 1 0.438557 2 2 -0.225821 3 3 -1.194444 4 female 2.694665 5 under15 1.679459 6 family -0.322922 4.删除Fare12345678# test4, no FAREtrain_ft_4=train.drop(['Survived','Fare'],axis=1)test_4 = test.drop(['Fare'],axis=1)train_ft.head()# mlkf = KFold(n_splits=3,random_state=1)acc_lst = []ml(train_ft_4,train_y,'test_4') Features Coefficient Estimate 0 1 0.564754 1 2 -0.242384 2 3 -1.287715 3 female 2.699738 4 under15 1.629584 5 young 0.058133 6 family -0.269146 7 C 0.436600 5.删除C12345678# test5, get rid of c train_ft_5=train.drop(['Survived','C'],axis=1)test_5 = test.drop('C',axis=1)# mlkf = KFold(n_splits=3,random_state=1)acc_lst = []ml(train_ft_5,train_y,'test_5') Features Coefficient Estimate 0 Fare 0.004841 1 1 0.442430 2 2 -0.232150 3 3 -1.207308 4 female 2.691465 5 under15 1.700077 6 young 0.052091 7 family -0.320831 6.删除Fare和young12345678# test6, lose Fare and youngtrain_ft_6=train.drop(['Survived','Fare','young'],axis=1)test_6 = test.drop(['Fare','young'],axis=1)train_ft.head()# mlkf = KFold(n_splits=3,random_state=1)acc_lst = []ml(train_ft_6,train_y,'test_6') Features Coefficient Estimate 0 1 0.562814 1 2 -0.235606 2 3 -1.274657 3 female 2.702955 4 under15 1.604597 5 family -0.270284 6 C 0.442288 结果汇总1234accuracy_df=pd.DataFrame(data=accuracy, index=[&apos;test1&apos;,&apos;test2&apos;,&apos;test3&apos;,&apos;test4&apos;,&apos;test5&apos;,&apos;test6&apos;], columns=[&apos;logistic&apos;,&apos;rf&apos;,&apos;svc&apos;,&apos;knn&apos;])accuracy_df 确定模型和特征综合来看，test_4和支持向量机的表现最好，所以用该模型进行预测123456789svc = SVC()svc.fit(train_ft_4,train_y)svc_pred = svc.predict(test_4)print(svc.score(train_ft_4,train_y))submission_test = pd.read_csv("test.csv")submission = pd.DataFrame(&#123;"PassengerId":submission_test['PassengerId'], "Survived":svc_pred&#125;)submission.to_csv("kaggle_SVC.csv",index=False) 0.832772166105 结果提交 ReferenceTitanic: Machine Learning from Disaster TitanicLearningQI]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[特征工程]]></title>
      <url>%2F2017%2F04%2F24%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[特征使用方案确定需要的数据需要相关业务知识和领域知识，尽可能找出对因变量有影响的所有自变量 可用性评估获取难度、覆盖率、准确率 特征获取方案获取方式系统或日志里的数据需要网上爬取的数据 存储方式文本格式、CSV格式、关系表、键值对数据库的选择：关系数据库（MySql，Oracle），Nosql数据库（mongoDB，Redis） 特征处理数据清洗 异常处理 数据采样 预处理单个特征 归一化 离散化 Dummy Coding 缺失值处理 数据变换 log 指数 Box-Cox 特征降维 特征抽取PCALDALLE拉普拉斯映射 特征选择FilterWapperEmbedded 特征组合对特征进行加工，生成线性、非线性组合，提高模型表现 特征监控特征有效性分析特征重要性，权重 特征监控防止特征质量下降，影响模型效果 Reference使用sklearn做单机特征工程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo个人博客搭建教程]]></title>
      <url>%2F2017%2F04%2F21%2FHexo%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[依赖包安装1. git安装在ubuntu系统下，直接在命令行输入1sudo apt-get install git-core 2. node.js安装直接从官网下载的预编译安装包，解压后需要设置全局变量，按着网上教程，最后在安装完Hexo后，系统找不到Hexo命令推荐使用Hexo官网的方法来，首先安装nvmcURL:1$ curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh Wget:1$ wget -qO- https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 然后用nvm安装1$ nvm install stable 3. Hexo安装利用npm安装1$ npm install -g hexo-cli 初始化设置cd到博客安装的文件夹，执行12hexo initnpm install 可以看到博客初始化完成了，接着可以新建博客了，在博客根目录下执行1hexo new “My First Blog” 此时在 blog/source/_post/ 文件夹下生成了My First Blog.md文件，以后写博文都是在文件里进行的，然后执行12hexo generatehexo server 在浏览器打开 localhost://4000 可查看博客页面如果需要重新发布，按顺序执行123hexo cleanhexo ghexo s 初步搭建就完成了，下一步可进行个性化设置，比如主题更改和链接到Github]]></content>
    </entry>

    
  
  
</search>
